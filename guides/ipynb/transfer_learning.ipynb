{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer_learning",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXW75uDNvkTM"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM-t8jl0vkTY"
      },
      "source": [
        "## An end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset\n",
        "\n",
        "To solidify these concepts, let's walk you through a concrete end-to-end transfer\n",
        "learning & fine-tuning example. We will load the Xception model, pre-trained on\n",
        " ImageNet, and use it on the Kaggle \"cats vs. dogs\" classification dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wleOz82hvkTY"
      },
      "source": [
        "### Getting the data\n",
        "\n",
        "First, let's fetch the cats vs. dogs dataset using TFDS. If you have your own dataset,\n",
        "you'll probably want to use the utility\n",
        "`tf.keras.preprocessing.image_dataset_from_directory` to generate similar labeled\n",
        " dataset objects from a set of images on disk filed into class-specific folders.\n",
        "\n",
        "Transfer learning is most useful when working with very small datasets. To keep our\n",
        "dataset small, we will use 40% of the original training data (25,000 images) for\n",
        " training, 10% for validation, and 10% for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMbr0tM5vkTY"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "train_ds, validation_ds, test_ds = tfds.load(\n",
        "    \"cats_vs_dogs\",\n",
        "    # Reserve 10% for validation and 10% for test\n",
        "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n",
        "    as_supervised=True,  # Include labels\n",
        ")\n",
        "\n",
        "print(\"Number of training samples: %d\" % tf.data.experimental.cardinality(train_ds))\n",
        "print(\n",
        "    \"Number of validation samples: %d\" % tf.data.experimental.cardinality(validation_ds)\n",
        ")\n",
        "print(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_ds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg0m1m6EvkTZ"
      },
      "source": [
        "These are the first 9 images in the training dataset -- as you can see, they're all\n",
        " different sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rFNUJJHvkTZ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i, (image, label) in enumerate(train_ds.take(9)):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(int(label))\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL-R7wXUvkTZ"
      },
      "source": [
        "We can also see that label 1 is \"dog\" and label 0 is \"cat\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZesMqOkvkTZ"
      },
      "source": [
        "### Standardizing the data\n",
        "\n",
        "Our raw images have a variety of sizes. In addition, each pixel consists of 3 integer\n",
        "values between 0 and 255 (RGB level values). This isn't a great fit for feeding a\n",
        " neural network. We need to do 2 things:\n",
        "\n",
        "- Standardize to a fixed image size. We pick 150x150.\n",
        "- Normalize pixel values between -1 and 1. We'll do this using a `Normalization` layer as\n",
        " part of the model itself.\n",
        "\n",
        "In general, it's a good practice to develop models that take raw data as input, as\n",
        "opposed to models that take already-preprocessed data. The reason being that, if your\n",
        "model expects preprocessed data, any time you export your model to use it elsewhere\n",
        "(in a web browser, in a mobile app), you'll need to reimplement the exact same\n",
        "preprocessing pipeline. This gets very tricky very quickly. So we should do the least\n",
        " possible amount of preprocessing before hitting the model.\n",
        "\n",
        "Here, we'll do image resizing in the data pipeline (because a deep neural network can\n",
        "only process contiguous batches of data), and we'll do the input value scaling as part\n",
        " of the model, when we create it.\n",
        "\n",
        "Let's resize images to 150x150:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9SvlZnxvkTa"
      },
      "source": [
        "size = (150, 150)\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
        "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
        "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izg7jDI0vkTa"
      },
      "source": [
        "Besides, let's batch the data and use caching & prefetching to optimize loading speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sZLyx0vvkTa"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "validation_ds = validation_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX8PsOgFvkTa"
      },
      "source": [
        "### Using random data augmentation\n",
        "\n",
        "When you don't have a large image dataset, it's a good practice to artificially\n",
        " introduce sample diversity by applying random yet realistic transformations to\n",
        "the training images, such as random horizontal flipping or small random rotations. This\n",
        "helps expose the model to different aspects of the training data while slowing down\n",
        " overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFNmSSfSvkTb"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
        "    ]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPacWf_2vkTb"
      },
      "source": [
        "Let's visualize what the first image of the first batch looks like after various random\n",
        " transformations:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4qFs_6dvkTb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "for images, labels in train_ds.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    first_image = images[0]\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        augmented_image = data_augmentation(\n",
        "            tf.expand_dims(first_image, 0), training=True\n",
        "        )\n",
        "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpWb0cqbvkTc"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "Now let's built a model that follows the blueprint we've explained earlier.\n",
        "\n",
        "Note that:\n",
        "\n",
        "- We add a `Normalization` layer to scale input values (initially in the `[0, 255]`\n",
        " range) to the `[-1, 1]` range.\n",
        "- We add a `Dropout` layer before the classification layer, for regularization.\n",
        "- We make sure to pass `training=False` when calling the base model, so that\n",
        "it runs in inference mode, so that batchnorm statistics don't get updated\n",
        "even after we unfreeze the base model for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r9Jy2QZvkTc"
      },
      "source": [
        "base_model = keras.applications.Xception(\n",
        "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False,\n",
        ")  # Do not include the ImageNet classifier at the top.\n",
        "\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create new model on top\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = data_augmentation(inputs)  # Apply random data augmentation\n",
        "\n",
        "# Pre-trained Xception weights requires that input be normalized\n",
        "# from (0, 255) to a range (-1., +1.), the normalization layer\n",
        "# does the following, outputs = (inputs - mean) / sqrt(var)\n",
        "norm_layer = keras.layers.experimental.preprocessing.Normalization()\n",
        "mean = np.array([127.5] * 3)\n",
        "var = mean ** 2\n",
        "# Scale inputs to [-1, +1]\n",
        "x = norm_layer(x)\n",
        "norm_layer.set_weights([mean, var])\n",
        "\n",
        "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
        "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
        "# base_model is running in inference mode here.\n",
        "x = base_model(x, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsF9LYp6vkTc"
      },
      "source": [
        "## Train the top layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAlUANxGvkTd"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SaBRlUpvkTd"
      },
      "source": [
        "## Do a round of fine-tuning of the entire model\n",
        "\n",
        "Finally, let's unfreeze the base model and train the entire model end-to-end with a low\n",
        " learning rate.\n",
        "\n",
        "Importantly, although the base model becomes trainable, it is still running in\n",
        "inference mode since we passed `training=False` when calling it when we built the\n",
        "model. This means that the batch normalization layers inside won't update their batch\n",
        "statistics. If they did, they would wreck havoc on the representations learned by the\n",
        " model so far."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRvN7aSlvkTd"
      },
      "source": [
        "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
        "# since we passed `training=False` when calling it. This means that\n",
        "# the batchnorm layers will not update their batch statistics.\n",
        "# This prevents the batchnorm layers from undoing all the training\n",
        "# we've done so far.\n",
        "base_model.trainable = True\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.BinaryAccuracy()],\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC7MGTPQvkTe"
      },
      "source": [
        "After 10 epochs, fine-tuning gains us a nice improvement here."
      ]
    }
  ]
}